{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Tweets\n",
    "* introducing custom analyzers in the settings segments of ES index\n",
    "* prepare settings and mappings for the index\n",
    "* (bulk) inserting tweets using elasticsearch api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command is used to install elasticsearch library.\n",
    "# Use it once and comment it out again\n",
    "#!pip install elasticsearch\n",
    "\n",
    "\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "\n",
    "#name of the created index\n",
    "index_name = \"tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom analyzers in ES\n",
    "In ES, the textual elements of docuements can be analyzed before being indexed. This analysis process can be done by the default \"standard\" analyzer, or developer can build his own custom analyzer in the setting segment.\n",
    "To have a better idea about what analyzer can do with text, try out the following analyzer using Kibana Dev. tools.\n",
    "\n",
    "__Example-1__\n",
    "```json\n",
    "GET /_analyze \n",
    "{\n",
    "  \"tokenizer\": \"whitespace\",\n",
    "  \"filter\": [\"lowercase\", \"stop\"],\n",
    "  \"char_filter\": [\"html_strip\"],\n",
    "  \"text\": \"text to be analyzed. It contains <html></html>\"\n",
    "}\n",
    "```\n",
    "__Example-2__\n",
    "```json\n",
    "GET /_analyze \n",
    "{\n",
    "  \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "             {\n",
    "              \"type\": \"ngram\",\n",
    "              \"min_gram\": 3,\n",
    "              \"max_gram\": 4\n",
    "            }\n",
    "          ],\n",
    "  \"char_filter\": [\"html_strip\"],\n",
    "  \"text\": \"text to be analyzed. It contains <html></html>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'tweets'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index with settings and mapping\n",
    "\n",
    "# This test is done during development only. \n",
    "if es.indices.exists(index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    \n",
    "\n",
    "# index settings\n",
    "settings = {\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"filter\": {\n",
    "        \"trigrams_filter\": {\n",
    "          \"type\": \"ngram\",\n",
    "          \"min_gram\": 3,\n",
    "          \"max_gram\": 4\n",
    "        }\n",
    "      },\n",
    "      \"analyzer\": {\n",
    "        \"text_processing\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "            \"lowercase\",\n",
    "            \"trigrams_filter\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    ",\"mappings\": {\n",
    "        \"properties\": {\n",
    "          \"date\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"flag\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"id\": {\n",
    "          \"type\": \"keyword\",\n",
    "            \"ignore_above\": 256\n",
    "        },\n",
    "        \"target\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"text\": {\n",
    "          \"type\": \"text\"\n",
    "        },\n",
    "        \"user\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "        }\n",
    "\n",
    "}\n",
    "\n",
    "    }\n",
    "# create index\n",
    "es.indices.create(index=index_name, ignore=400, body=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source:\n",
    "https://www.kaggle.com/kazanova/sentiment140/data#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_index': 'tweets', '_type': '_doc', '_id': '2193602064', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n"
     ]
    }
   ],
   "source": [
    "# inserting records\n",
    "\n",
    "# target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "# ids: The id of the tweet ( 2087)\n",
    "# date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "# flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "# user: the user that tweeted (robotickilldozr)\n",
    "# text: the text of the tweet (Lyx is cool)\n",
    "    \n",
    "\n",
    "tweet = {\n",
    "  \"target\": \"4\",\n",
    "  \"id\": \"2193602064\",\n",
    "  \"date\": \"Tue Jun 16 08:40:49 PDT 2009\",\n",
    "  \"flag\": \"NO_QUERY\",\n",
    "  \"user\": \"tinydiamondz\",\n",
    "  \"text\": \"Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur\"\n",
    "}\n",
    "\n",
    "res = es.index(index=index_name, id=tweet['id'], body=tweet)\n",
    "\n",
    "\n",
    "\n",
    "print(res)\n",
    "\n",
    "# Now check http://localhost:9200/tweets/_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import helpers\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "i = 1\n",
    "actions = []\n",
    "with gzip.open('training.1600000.processed.noemoticon.csv.gz','rt',encoding='latin-1') as f:\n",
    "    #print(i, len(actions))\n",
    "    for line in f:\n",
    "        if i%10000!=0:\n",
    "            #print('got line', i)\n",
    "            line = line.replace(\"\\\"\", \"\")\n",
    "            line = line.split(\",\")\n",
    "            tweet = {\n",
    "              \"target\": line[0],\n",
    "              \"id\": line[1],\n",
    "              \"date\": line[2],\n",
    "              \"flag\": line[3],\n",
    "              \"user\": line[4],\n",
    "              \"text\": line[5]\n",
    "            }\n",
    "            actions.append(tweet)\n",
    "        else:\n",
    "            try:\n",
    "                line = line.replace(\"\\\"\", \"\")\n",
    "                line = line.split(\",\")\n",
    "                tweet = {\n",
    "                  \"target\": line[0],\n",
    "                  \"id\": line[1],\n",
    "                  \"date\": line[2],\n",
    "                  \"flag\": line[3],\n",
    "                  \"user\": line[4],\n",
    "                  \"text\": line[5]\n",
    "                }\n",
    "                actions.append(tweet)\n",
    "                helpers.bulk(es, actions,index=index_name)\n",
    "                actions = []\n",
    "            except:\n",
    "                None\n",
    "        i=i+1\n",
    "        \n",
    "        \n",
    "        \n",
    "# Practice:\n",
    "    #1) download the twitter file and compress it using gzip command\n",
    "    #2) read the tweets oe by one using the above code, then complete it towards performing the follwoing:\n",
    "        # a) create a json tweet and add it to actions list\n",
    "        # b) bulk insert each 10000 into \"tweets\" index\n",
    "    #3) [otional] repeate all the above steps using 'scala'\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
